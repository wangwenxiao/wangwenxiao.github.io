---
layout: home
profile_picture:
  src: /assets/img/wwx.jpeg
  alt: website picture
---


Hi! I am Wenxiao Wang (汪文潇), currently the Head of AI at [RELAI, Inc](https://relai.ai/).
I obtained my Ph.D. in Computer Science from University of Maryland in 2025, advised by Prof. [Soheil Feizi](https://www.cs.umd.edu/~sfeizi/). Prior to that I obtained my B.S. in Computer Science from [Yao Class](https://iiis.tsinghua.edu.cn/en/yaoclass/), Tsinghua University in 2020.

I was a research intern at Sony AI (summer 2023), working with Dr. [Weiming Zhuang](https://weiming.me) and Dr. [Lingjuan Lyu](https://sites.google.com/view/lingjuan-lyu/home?pli=1) in Privacy-Preserving Machine Learning (PPML) team; I was a research intern at Bytedance (summer 2022), working with Dr. [Linjie Yang](https://sites.google.com/site/linjieyang89/), Dr. [Heng Wang](https://hengcv.github.io) and Dr. [Yu Tian](https://scholar.google.com/citations?user=DxPjkDoAAAAJ); a research assistant at IIIS, Tsinghua University (2020-2021), working with Prof. [Hang Zhao](https://hangzhaomit.github.io) in his [MARS Lab](http://group.iiis.tsinghua.edu.cn/~marslab/#/); a visiting student researcher at UC Berkeley (2019), working with Dr. [Xinyun Chen](https://jungyhuk.github.io), Prof. [Ruoxi Jia](https://ruoxijia.info) and Prof. [Dawn Song](https://people.eecs.berkeley.edu/~dawnsong/); an intern in Bytedance AI Lab (2018), working with Dr. Yi He and Prof. [Lei Li](https://sites.cs.ucsb.edu/~lilei/).

## Preprints

**Maestro: Joint Graph & Config Optimization for Reliable AI Agents**  
**<u>Wenxiao Wang</u>**, Priyatham Kattakinda and Soheil Feizi  
[[arxiv]](https://arxiv.org/abs/2509.04642)

**Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption**  
**<u>Wenxiao Wang</u>**, Parsa Hosseini and Soheil Feizi  
[[arxiv]](https://arxiv.org/abs/2504.20769)

**Towards Fundamentally Scalable Model Selection: Asymptotically Fast Update and Selection**  
**<u>Wenxiao Wang</u>**, Weiming Zhuang and Lingjuan Lyu  
[[arxiv]](https://arxiv.org/abs/2406.07536)


## Publications

**Towards Reliable Agentic LLMs**  
**<u>Wenxiao Wang</u>**  
Ph.D. Dissertation, 2025.  
[[paper]](https://www.proquest.com/docview/3250301036)

**DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors**  
Yize Cheng\*, **<u>Wenxiao Wang</u>**\*, Mazda Moayeri and Soheil Feizi (\*equal contribution)  
Conference on Empirical Methods in Natural Language Processing (**EMNLP**), 2025.  
[[paper]](https://arxiv.org/abs/2505.23001)

**Tool Preferences in Agentic LLMs are Unreliable**  
Kazem Faghih\*, **<u>Wenxiao Wang</u>**\*, Yize Cheng\*, Siddhant Bharti, Gaurang Sriramanan, Sriram Balasubramanian, Parsa Hosseini and Soheil Feizi (\*equal contribution)  
Conference on Empirical Methods in Natural Language Processing (**EMNLP**), 2025.  
[[paper]](https://arxiv.org/abs/2505.18135)

**Can AI-Generated Text be Reliably Detected?**  
Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, **<u>Wenxiao Wang</u>** and Soheil Feizi  
Media Coverage: 
[[Washington Post]](https://www.washingtonpost.com/technology/2023/06/02/turnitin-ai-cheating-detector-accuracy/)
[[Wired]](https://www.wired.com/story/ai-detection-chat-gpt-college-students/)
[[New Scientist]](https://www.newscientist.com/article/2366824-reliably-detecting-ai-generated-text-is-mathematically-impossible/)
[[The Register]](https://www.theregister.com/2023/03/21/detecting_ai_generated_text/?utm_source=twitter&utm_medium=twitter&utm_campaign=auto&utm_content=article)
[[TechSpot]](https://www.techspot.com/news/98031-reliable-detection-ai-generated-text-impossible-new-study.html)
[[UMD Science]](https://cmns.umd.edu/news-events/news/ai-generated-content-actually-detectable)  
Transactions on Machine Learning Research (**TMLR**), 2025.  
[[paper]](https://arxiv.org/abs/2303.11156)

**Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks**  
Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, **<u>Wenxiao Wang</u>** and Soheil Feizi  
Media Coverage: 
[[Wired]](https://www.wired.com/story/artificial-intelligence-watermarking-issues/)
[[MIT Tech Review]](https://www.technologyreview.com/2023/11/06/1082996/the-inside-scoop-on-watermarking-and-content-authentication/)
[[Bloomberg News]](https://www.bloomberg.com/news/newsletters/2023-11-06/biden-ai-executive-order-shows-urgency-of-deepfakes)
[[The Register]](https://www.theregister.com/2023/10/02/watermarking_security_checks/)  
International Conference on Learning Representations (**ICLR**), 2024.  
[[paper]](https://arxiv.org/abs/2310.00076)

**DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness**  
Shoumik Saha, **<u>Wenxiao Wang</u>**, Yigitcan Kaya, Soheil Feizi and Tudor Dumitras  
International Conference on Learning Representations (**ICLR**), 2024.  
[[paper]](https://arxiv.org/abs/2303.13372)

**Temporal Robustness against Data Poisoning**  
**<u>Wenxiao Wang</u>** and Soheil Feizi  
Conference on Neural Information Processing Systems (**NeurIPS**), 2023.  
[[paper]](https://arxiv.org/abs/2302.03684)

**Spuriosity Rankings: Sorting Data for Spurious Correlation Robustness**  
Mazda Moayeri, **<u>Wenxiao Wang</u>**, Sahil Singla and Soheil Feizi  
Conference on Neural Information Processing Systems (**NeurIPS**), 2023. [**spotlight**]  
[[paper]](https://arxiv.org/abs/2212.02648)

**Lethal Dose Conjecture on Data Poisoning**  
**<u>Wenxiao Wang</u>**, Alexander Levine and Soheil Feizi  
Conference on Neural Information Processing Systems (**NeurIPS**), 2022.  
[[paper]](https://arxiv.org/abs/2208.03309) [[code]](https://github.com/wangwenxiao/FiniteAggregation)

**Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation**  
**<u>Wenxiao Wang</u>**, Alexander Levine and Soheil Feizi  
International Conference on Machine Learning (**ICML**), 2022.  
[[paper]](https://proceedings.mlr.press/v162/wang22m.html) [[code]](https://github.com/wangwenxiao/FiniteAggregation)  

**On Feature Decorrelation in Self-Supervised Learning**  
Tianyu Hua\*, **<u>Wenxiao Wang</u>**\*, Zihui Xue, Sucheng Ren, Yue Wang, Hang Zhao (\*equal contribution)  
International Conference on Computer Vision (**ICCV**), 2021. [**oral**]  
[[paper]](https://openaccess.thecvf.com/content/ICCV2021/html/Hua_On_Feature_Decorrelation_in_Self-Supervised_Learning_ICCV_2021_paper.html) [[code]](https://github.com/PatrickHua/FeatureDecorrelationSSL)  

**DPlis: Boosting Utility of Differentially Private Deep Learning via Randomized Smoothing**  
**<u>Wenxiao Wang</u>**, Tianhao Wang, Lun Wang, Nanqing Luo, Pan Zhou, Dawn Song, Ruoxi Jia  
Privacy Enhancing Technologies Symposium (**PETS**), 2021.  
[[paper]](https://www.petsymposium.org/2021/files/papers/issue4/popets-2021-0065.pdf) [[code]](https://github.com/wangwenxiao/DPlis)

**REFIT: A Unified Watermark Removal Framework For Deep Learning Systems With Limited Data**  
Xinyun Chen\*, **<u>Wenxiao Wang</u>**\*, Yiming Ding, Chris Bender, Ruoxi Jia, Bo Li, Dawn Song (\*equal contribution)  
ACM Asia Conference on Computer and Communications Security (**AsiaCCS**), 2021.  
[[paper]](https://dl.acm.org/doi/abs/10.1145/3433210.3453079) [[code]](https://github.com/sunblaze-ucb/REFIT)

**The Secret Revealer: Generative Model Inversion Attacks Against Deep Neural Networks**  
Yuheng Zhang\*, Ruoxi Jia\*, Hengzhi Pei, **<u>Wenxiao Wang</u>**, Bo Li, Dawn Song (\*equal contribution)  
Conference on Computer Vision and Pattern Recognition (**CVPR**), 2020. [**oral**]  
[[paper]](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_The_Secret_Revealer_Generative_Model-Inversion_Attacks_Against_Deep_Neural_Networks_CVPR_2020_paper.html) [[code]](https://github.com/AI-secure/GMI-Attack)

**Leveraging Unlabeled Data for Watermark Removal of Deep Neural Networks**  
Xinyun Chen\*, **<u>Wenxiao Wang</u>**\*, Yiming Ding, Chris Bender, Ruoxi Jia, Bo Li, Dawn Song (\*equal contribution)  
ICML Workshop on Security and Privacy of Machine Learning, 2019.  
[[paper]](https://wangwenxiao.github.io/files/watermark_removal_icml19_workshop.pdf)

## Talks
- **Temporal Robustness against Data Poisoning**, AI TIME Youth PhD Talk, November 2023.
- **Lethal Dose Conjecture: From Few-shot Learning to Potentially Nearly Optimal Defenses
against Data Poisoning**, TMLR Group, Hong Kong Baptist University, December 2022.
- **Lethal Dose Conjecture on Data Poisoning**, AI TIME Youth PhD Talk, November 2022.
- **Improved Certified Defenses against Data Poisoning with (Deterministic) Finite Aggregation**, AI TIME Youth PhD Talk, August 2022.


## Services
Reviewer of: NeurIPS, ICML, ICLR, ICCV, CVPR, TPAMI, TMLR, ...
